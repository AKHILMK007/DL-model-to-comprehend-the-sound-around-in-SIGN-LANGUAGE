{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzK23B0Fp4vSTnwkAOu9Ui"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1Jh0tVfdOqyh"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import cv2\n","import os\n","import glob"]},{"cell_type":"code","source":["# initial parameters\n","epochs = 100\n","lr = 1e-3\n","batch_size = 64\n","img_dims = (96,96,3)"],"metadata":{"id":"AuPI5iplO2_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = []\n","labels = []"],"metadata":{"id":"YS0WyN-XO9i3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load image files from the dataset\n","image_files = [f for f in glob.glob(r'C:\\Files\\gender_dataset_face' + \"/**/*\", recursive=True) if not os.path.isdir(f)]\n","random.shuffle(image_files)"],"metadata":{"id":"2QDzx-w8PDV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# converting images to arrays and labelling the categories\n","for img in image_files:\n","\n","    image = cv2.imread(img)\n","    \n","    image = cv2.resize(image, (img_dims[0],img_dims[1]))\n","    image = img_to_array(image)\n","    data.append(image)\n","\n","    label = img.split(os.path.sep)[-2] # C:\\Files\\gender_dataset_face\\woman\\face_1162.jpg\n","    if label == \"woman\":\n","        label = 1\n","    else:\n","        label = 0\n","        \n","    labels.append([label]) # [[1], [0], [0], ...]\n"],"metadata":{"id":"CGdTyOyAPM9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pre-processing\n","data = np.array(data, dtype=\"float\") / 255.0\n","labels = np.array(labels)"],"metadata":{"id":"0u5dLccePlQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split dataset for training and validation\n","(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.33,\n","                                                  random_state=42)\n","\n","trainY = to_categorical(trainY, num_classes=2) # [[1, 0], [0, 1], [0, 1], ...]\n","testY = to_categorical(testY, num_classes=2)\n"],"metadata":{"id":"XXPAlXpJPsb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# augmenting datset \n","aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n","                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n","                         horizontal_flip=True, fill_mode=\"nearest\")\n"],"metadata":{"id":"d1UwXP9uPwYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define model\n","def build(width, height, depth, classes):\n","    model = Sequential()\n","    inputShape = (height, width, depth)\n","    chanDim = -1\n","\n","    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n","        inputShape = (depth, height, width)\n","        chanDim = 1\n","    \n","    # The axis that should be normalized, after a Conv2D layer with data_format=\"channels_first\", \n","    # set axis=1 in BatchNormalization.\n","\n","    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=chanDim))\n","    model.add(MaxPooling2D(pool_size=(3,3)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(64, (3,3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=chanDim))\n","\n","    model.add(Conv2D(64, (3,3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=chanDim))\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(128, (3,3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=chanDim))\n","\n","    model.add(Conv2D(128, (3,3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=chanDim))\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Flatten())\n","    model.add(Dense(1024))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(classes))\n","    model.add(Activation(\"sigmoid\"))\n","\n","    return model"],"metadata":{"id":"LpF531YLP0gl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build model\n","model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n","                            classes=2)"],"metadata":{"id":"3gDqmb-YP9Q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compile the model\n","opt = Adam(lr=lr, decay=lr/epochs)\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"],"metadata":{"id":"c9wtA2jpQBWF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the model\n","H = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n","                        validation_data=(testX,testY),\n","                        steps_per_epoch=len(trainX) // batch_size,\n","                        epochs=epochs, verbose=1)\n"],"metadata":{"id":"piruqF48QEUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model to disk\n","model.save('gender_detection.model')"],"metadata":{"id":"GjHOEewFQHGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot training/validation loss/accuracy\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","N = epochs\n","plt.plot(np.arange(0,N), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0,N), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0,N), H.history[\"acc\"], label=\"train_acc\")\n","plt.plot(np.arange(0,N), H.history[\"val_acc\"], label=\"val_acc\")\n","\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"upper right\")"],"metadata":{"id":"jVNKvMUmQLtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save plot to disk\n","plt.savefig('plot.png')"],"metadata":{"id":"r2I8IKMiQQYV"},"execution_count":null,"outputs":[]}]}